{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "from functools import lru_cache, partial\n",
    "from pprint import pprint\n",
    "\n",
    "import gym_trading_env\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import MultiTrade\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gym_trading_env.downloader import download\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "from gym_trading_env.renderer import Renderer\n",
    "from IPython.display import display\n",
    "from ray import train, tune\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.utils import build_dataset, build_market_image,preprocess_data,stack_arrays\n",
    "from utils.forecast_utils import create_ts_preprocessor,create_ts_dataset\n",
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from MultiTrade.environments import ForecastingTradingEnv,NeuralForecastingTradingEnv\n",
    "\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.util import select_by_index\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSTConfig,\n",
    "    PatchTSTForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.deep_q_learning import DeepQLearning\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import OneHotActionTensorRepresentationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COIN_PAIRS=['BTC/USDT','ETH/USDT','SOL/USDT','BNB/USDT','XRP/USDT','ADA/USDT',\n",
    "            'ETH/BTC','SOL/ETH','BNB/ETH','XRP/ETH',\"ADA/ETH\",\n",
    "            'SOL/BTC','SOL/BNB',\n",
    "            'XRP/BTC','XRP/BNB',\n",
    "            'ADA/BTC','ADA/BNB',\n",
    "            ]\n",
    "target_pair='ETHUSDT'\n",
    "time_frame=\"1h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download(exchange_names = [\"binance\"],\n",
    "#     symbols= tqdm(COIN_PAIRS),\n",
    "#     timeframe= time_frame,\n",
    "#     dir = \"data\",\n",
    "#     since= datetime.datetime(year= 2022, month= 1, day=1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PatchTSTForPrediction.from_pretrained(\"C:/Users/standard/Git/MultiTrader/forecaster_pretrain/output/checkpoint-19392\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=datetime.datetime(year= 2024, month= 2, day=1)\n",
    "split_date=start_date+datetime.timedelta(days=30)\n",
    "end_date=split_date+datetime.timedelta(days=30)\n",
    "target_pair='ETH/USDT'\n",
    "data=build_market_image(target_pair=target_pair,time_frame='1h',axis=0)\n",
    "data=data[data['symbol']==target_pair.replace('/','')].copy()\n",
    "data.index\n",
    "# hf_data=data.copy()\n",
    "\n",
    "# hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "# hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "\n",
    "# hf_train_data=hf_train_data.reset_index(level=0,drop=True).reset_index()\n",
    "# prepper=create_ts_preprocessor(hf_train_data)\n",
    "# hf_train_data=prepper.preprocess(hf_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date=datetime.datetime(year= 2024, month= 3, day=1)\n",
    "end_date=split_date+datetime.timedelta(days=14)\n",
    "\n",
    "# data=build_market_image(target_pair='ETH/USDT',time_frame='1h')\n",
    "\n",
    "hf_data=data.copy()\n",
    "hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "hf_test_data=hf_data.groupby('symbol').apply(lambda x: x[split_date:end_date])\n",
    "hf_test_data=hf_test_data.reset_index(level=0,drop=True).reset_index()\n",
    "# hf_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_envs(data,symbol='ETHUSDT',look_back=7):\n",
    "    start_date=datetime.datetime(year= 2024, month= 1, day=1)\n",
    "    split_date=datetime.datetime(year= 2024, month= 3, day=1)\n",
    "    end_date=split_date+datetime.timedelta(days=14)\n",
    "\n",
    "    \n",
    "    hf_data=data.copy()\n",
    "    \n",
    "    hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "    hf_test_data=hf_data.groupby('symbol').apply(lambda x: x[split_date:end_date])\n",
    "\n",
    "    hf_train_data=hf_train_data.reset_index(level=0,drop=True).reset_index()\n",
    "    hf_test_data=hf_test_data.reset_index(level=0,drop=True).reset_index()\n",
    "\n",
    "\n",
    "    trade_data=data[data['symbol']==symbol].copy()\n",
    "    train_data=trade_data[:split_date]\n",
    "    test_data=trade_data[split_date:end_date]\n",
    "\n",
    "\n",
    "    train_env = NeuralForecastingTradingEnv(\n",
    "                                        model=model,\n",
    "\n",
    "                                        name='ETHUSDT_train',\n",
    "                                        df = train_data, # Your dataset with your custom features\n",
    "                                        positions = [ -.25, 0, .25], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "                                        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "                                        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "                                        max_episode_duration=168,\n",
    "                                        verbose=0\n",
    "\n",
    "                                        )\n",
    "    \n",
    "    test_env = ForecastingTradingEnv(\n",
    "                                        model=model,\n",
    "                                        hf_data=hf_test_data,\n",
    "                                        \n",
    "                                        name='ETHUSDT_test',\n",
    "                                        df = test_data, # Your dataset with your custom features\n",
    "                                        positions = [ -.25, 0, .25], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "                                        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "                                        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "                                        max_episode_duration=168,\n",
    "                                        # verbose=0\n",
    "                                        \n",
    "                                    )\n",
    "    return train_env,test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_env,test_env=get_train_test_envs(data,look_back=7)\n",
    "train_pearl_env=GymEnvironment(train_env)\n",
    "test_pearl_env=GymEnvironment(test_env)\n",
    "# obs,info=test_env.reset()\n",
    "# obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=train_env.reset()\n",
    "# env.action_space.sa\n",
    "done=False\n",
    "while not done:\n",
    "    act=train_pearl_env.action_space.sample()\n",
    "    act_result=train_pearl_env.step(act)\n",
    "    done=act_result.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs=train_env.reset()\n",
    "# # env.action_space.sa\n",
    "# for i in range(20):\n",
    "#     act=train_env.action_space.sample()\n",
    "#     act_result=train_env.step(act)\n",
    "# act_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space={\n",
    "        # \"look_back\" : tune.choice([7,14,21,30,45,60]),\n",
    "\n",
    "        \"hidden_dims\" : tune.choice([[64,64],[128,128],[256,256]]),\n",
    "\n",
    "        'learning_rate':tune.uniform(1e-6, 1e-2),\n",
    "\n",
    "        'discount_factor': tune.uniform(1e-6, 1),\n",
    "\n",
    "        'training_rounds': tune.choice([c for c in range(2,64,2)]),\n",
    "\n",
    "        'batch_size': tune.choice([64,128,256]),\n",
    "        \n",
    "        'target_update_freq':tune.choice([c for c in range(2,64,2)]),\n",
    "\n",
    "        'soft_update_tau': tune.uniform(1e-6, 1),  # a value of 1 indicates no soft updates\n",
    "        \n",
    "        \"replay_buffer_size\":tune.choice([c for c in range(10,1_000,10)]),\n",
    "        }\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pearl(pearl_env):\n",
    "    naked_env=pearl_env.env.unwrapped\n",
    "    value_history=naked_env.historical_info['portfolio_valuation']\n",
    "    x=np.arange(len(value_history))\n",
    "    y=value_history\n",
    "    plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective(config):\n",
    "\n",
    "    hidden_dims=list(config[\"hidden_dims\"])\n",
    "    replay_buffer_size=config[\"replay_buffer_size\"]\n",
    "\n",
    "\n",
    "    # Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "    # Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "    # We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "    Q_network_DoubleDQN = VanillaQValueNetwork(state_dim=train_env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                                action_dim=train_env.action_space.n,                        # dimension of the action representation\n",
    "                                                hidden_dims=hidden_dims,                       # dimensions of the intermediate layers\n",
    "                                                output_dim=1)  \n",
    "    # Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "    # Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "    DoubleDQNagent = PearlAgent(\n",
    "                                policy_learner=DoubleDQN(\n",
    "                                                            state_dim=train_env.observation_space.shape[0],\n",
    "                                                            action_space=train_env.action_space,\n",
    "\n",
    "                                                            network_instance=Q_network_DoubleDQN,   # pass an instance of Q value network to the policy learner.\n",
    "                                                            action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "                                                                                                                                    max_number_actions=train_env.action_space.n\n",
    "                                                                                                                                ),\n",
    "                                                                                                                                \n",
    "                                                            **config\n",
    "                                                        ),\n",
    "                                replay_buffer=FIFOOffPolicyReplayBuffer(replay_buffer_size),\n",
    "                            ) \n",
    "    ## train dat bitch               \n",
    "    info = online_learning(\n",
    "                            agent=DoubleDQNagent ,\n",
    "                            env=train_pearl_env,\n",
    "                            number_of_episodes=20_000,\n",
    "                            print_every_x_episodes=100,   # print returns after every 10 episdoes\n",
    "                            learn_after_episode=True,    # updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "                            seed=0\n",
    "                            )\n",
    "    # plot_results(info)\n",
    "    agent=DoubleDQNagent\n",
    "    observation, action_space = test_pearl_env.reset()\n",
    "    agent.reset(observation, action_space)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(exploit=True)\n",
    "        action_result = test_pearl_env.step(action)\n",
    "        agent.observe(action_result)\n",
    "        agent.learn()\n",
    "        done = action_result.done\n",
    "\n",
    "    # plot_pearl(test_env)\n",
    "    score=action_result.info['portfolio_valuation']/1000\n",
    "    loss={\"score\": score,\n",
    "            \"_metric\": score}\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={x:y.sample() for x,y in search_space.items()}\n",
    "\n",
    "# objective(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trial_str_creator(trial):\n",
    "    return \"{}_{}_trading_agent\".format(trial.trainable_name, trial.trial_id)\n",
    "\n",
    "tune_config=tune.TuneConfig(num_samples=100,mode=\"max\",search_alg='hyperopt',\n",
    "                                trial_name_creator=trial_str_creator,\n",
    "                                trial_dirname_creator=trial_str_creator,\n",
    "\n",
    "                            )\n",
    "run_config=train.RunConfig(\n",
    "                            storage_path='C:/Users/standard/OneDrive/Documents/Git/MultiTrader/tune_results', \n",
    "                            name=\"DDQN_experiments\"\n",
    "                            )\n",
    "scaling_config=train.ScalingConfig(num_workers=4\n",
    "                                   \n",
    "                                   )\n",
    "# objective_with_resources = tune.with_resources(objective, {\"cpu\": 0.5})\n",
    "objective_with_resources = tune.with_resources(objective,{\"cpu\": 0.5,'gpu':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "else:\n",
    "    ray.shutdown()\n",
    "    ray.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner = tune.Tuner(objective_with_resources ,\n",
    "                   tune_config=tune_config,\n",
    "                   run_config=run_config,\n",
    "                #    scaling_config=scaling_config,\n",
    "                    param_space=search_space)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"max\").config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=results.get_best_result(metric=\"score\", mode=\"max\").config\n",
    "best_params\n",
    "# objective(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.env.unwrapped.save_for_render(dir = \"test_render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "renderer = Renderer(render_logs_dir = \"test_render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Custom Metrics (Annualized metrics)\n",
    "renderer.add_metric(\n",
    "    name = \"Annual Market Return\",\n",
    "    function = lambda df : f\"{ ((df['close'].iloc[-1] / df['close'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%\"\n",
    ")\n",
    "renderer.add_metric(\n",
    "        name = \"Annual Portfolio Return\",\n",
    "        function = lambda df : f\"{((df['portfolio_valuation'].iloc[-1] / df['portfolio_valuation'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%\"\n",
    ")\n",
    "\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
