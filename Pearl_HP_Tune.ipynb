{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_environments\n",
    "from utils import pearl_utils\n",
    "from configs import defaults\n",
    "from utils.reward_functions import log_reward_function,cumulative_reward_function,sharpe_reward_function\n",
    "from utils. utils import make_hidden_dims\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from Pearl.pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from Pearl.pearl.utils.functional_utils.train_and_eval.online_learning import \\\n",
    "    online_learning\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 4\n",
      "Seed set to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/binanceus-DOGEUSDT-1h.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 11197.06it/s]\n",
      "1it [00:00, 17.71it/s]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999c7121f7104fef8c7b52647bf3f5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feca8f299734f41ba9260e79c36d671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-22 15:00:00 2024-11-21 15:00:00\n"
     ]
    }
   ],
   "source": [
    "reward_functions=[log_reward_function,cumulative_reward_function,sharpe_reward_function]\n",
    "train_env,test_env=make_environments.make_envs(reward_function=log_reward_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(2), 'DOGEUSDT_train', (24,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp=train_env.positions\n",
    "train_env.action_space.n,train_env.name,train_env.observation_space.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 64, 64]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_hidden_dims(n_layers=3, n_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=pearl_utils.create_ddqn_model(\n",
    "\n",
    "        observation_space_dim=train_env.observation_space.shape[0], \n",
    "        action_space_dim=train_env.action_space.n,\n",
    "        hidden_dims=[64,64, 64], \n",
    "        training_rounds=20,\n",
    "        learning_rate = 0.001,\n",
    "        discount_factor = 0.99,\n",
    "        batch_size = 128,\n",
    "        target_update_freq = 10,\n",
    "        soft_update_tau = 0.75,  # a value of 1 indicates no soft updates\n",
    "        is_conservative = False,\n",
    "        conservative_alpha = False,\n",
    "        replay_buffer_size = 10_000,\n",
    "        lstm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24,), np.int64(2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.observation_space.shape,train_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=GymEnvironment(train_env)\n",
    "\n",
    "obs,action_space=env.reset()\n",
    "agent.reset(   obs, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done = False\n",
    "# while not done:\n",
    "#     action = agent.act(exploit=False)\n",
    "#     action_result = env.step(action)\n",
    "    \n",
    "#     agent.observe(action_result)\n",
    "#     loss=agent.learn()\n",
    "\n",
    "#     done = action_result.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c764de9da7c45d8bce44aea26b26a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info = online_learning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        # number_of_episodes=10,\n",
    "        number_of_steps=168,\n",
    "        print_every_x_episodes=2,   # print returns after every 10 episdoes\n",
    "        print_every_x_steps=1,   # print returns after every 10 episdoes\n",
    "        learn_every_k_steps=20,   # print returns after every 10 episdoes\n",
    "        learn_after_episode=False,\n",
    "        record_period=169,   # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "        seed=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective_function(trial):\n",
    " \n",
    "    reward_id=trial.suggest_categorical('reward_function', [0,1,2])\n",
    "    algo=trial.suggest_categorical('algorithm', ['dqn','ddqn'])    \n",
    "\n",
    "    # reward_id=0\n",
    "    \n",
    "    reward_func=reward_functions[reward_id]\n",
    "    train_env.reward_func=reward_func\n",
    "    test_env.reward_func=reward_func\n",
    "    \n",
    "    observation_space_dim=train_env.observation_space.shape[0]\n",
    "    action_space_dim=len(train_env.positions)\n",
    "    n_layers=trial.suggest_int('n_layers', 1, 3)\n",
    "    n_units=trial.suggest_categorical('n_units', [64,128,256,512])\n",
    "    \n",
    "    hidden_dims=make_hidden_dims(n_layers= n_layers, n_units=n_units)\n",
    "    \n",
    "    search_space={\n",
    "                'observation_space_dim': observation_space_dim,\n",
    "                'action_space_dim': action_space_dim,\n",
    "                'hidden_dims': hidden_dims,\n",
    "                'training_rounds': trial.suggest_int('training_rounds', 5, 30),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-4),\n",
    "                'discount_factor': trial.suggest_float('discount_factor', 0.8, 0.99), # gamma (greediness)\n",
    "                'batch_size': trial.suggest_categorical('batch_size', [64, 128]),\n",
    "                'target_update_freq': trial.suggest_categorical('target_update_freq', [1, 5, 10, 24]),\n",
    "                'soft_update_tau': trial.suggest_float('soft_update_tau', 0.1, .99), \n",
    "                'is_conservative': trial.suggest_categorical('is_conservative', [True, False]),\n",
    "                'lstm': trial.suggest_categorical('lstm', [True, False]),\n",
    "                'conservative_alpha': trial.suggest_float('conservative_alpha', 0.5, 1.0),\n",
    "                }\n",
    "\n",
    "    learning_space={'learn_after_episode':trial.suggest_categorical('learn_after_episode', [True, False]),\n",
    "                    'learning_steps':trial.suggest_int('learning_steps', 10, 89),\n",
    "                    'n_epochs':trial.suggest_categorical('n_epochs',[100,500]),\n",
    "                    }\n",
    "    #\n",
    "    # print('n_epochs',n_epochs)\n",
    "    if algo=='dqn':\n",
    "        agent=pearl_utils.create_dqn_model(**search_space)\n",
    "    elif algo=='ddqn':\n",
    "        agent=pearl_utils.create_ddqn_model(**search_space)\n",
    "\n",
    "        \n",
    "    agent=pearl_utils.train_pearl_model(agent,train_env,**learning_space)\n",
    "    profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "    objectives={'profit':profit,'n_trades':n_trades}\n",
    "\n",
    "    print('profit',profit,'n_trades',n_trades)\n",
    "\n",
    "    return profit,n_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=defaults.model_name\n",
    "model_name\n",
    "\n",
    "study_name=f\"{defaults.model_name}\"\n",
    "storage_name=\"sqlite:///PearlHPTuning.sqlite3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 14:45:55,036] Using an existing study with name 'DOGEUSDTSPOT' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study=create_study(study_name=study_name, \n",
    "             storage=storage_name, \n",
    "             load_if_exists=True,\n",
    "             directions=['maximize','maximize'],\n",
    "             sampler=TPESampler()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5153b55ff5694251b90247f4567b7ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [13:01<?, ?it/s]\n",
      "[W 2024-11-21 15:17:49,705] Trial 17 failed with parameters: {'reward_function': 2, 'algorithm': 'dqn', 'n_layers': 1, 'n_units': 64, 'training_rounds': 5, 'learning_rate': 3.8269015600878807e-05, 'discount_factor': 0.9443803480717309, 'batch_size': 64, 'target_update_freq': 24, 'soft_update_tau': 0.5818762981357136, 'is_conservative': True, 'lstm': True, 'conservative_alpha': 0.9760877905739401, 'learn_after_episode': True, 'learning_steps': 25, 'n_epochs': 500} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sebastiancoombs/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/nv/hm0pf_bn6gz3ql2py_lgdytc0000gp/T/ipykernel_85569/4208016341.py\", line 47, in objective_function\n",
      "    profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
      "  File \"/Users/sebastiancoombs/Documents/Git/MultiTrader/utils/pearl_utils.py\", line 162, in test_pearl_model\n",
      "    action = agent.act(exploit=True)\n",
      "  File \"/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl/pearl/pearl_agent.py\", line 157, in act\n",
      "    action = self.policy_learner.act(\n",
      "  File \"/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl/pearl/policy_learners/sequential_decision_making/deep_td_learning.py\", line 206, in act\n",
      "    with torch.no_grad():\n",
      "  File \"/Users/sebastiancoombs/anaconda3/envs/pearlenv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 154, in __new__\n",
      "    def __new__(cls, orig_func=None):\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-21 15:17:49,708] Trial 17 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective_function, n_trials\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     agent\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39mcreate_ddqn_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msearch_space)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m agent\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39mtrain_pearl_model(agent,train_env,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlearning_space)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m profit,n_trades\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39;49mtest_pearl_model(agent,test_env)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m objectives\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mprofit\u001b[39m\u001b[39m'\u001b[39m:profit,\u001b[39m'\u001b[39m\u001b[39mn_trades\u001b[39m\u001b[39m'\u001b[39m:n_trades}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_HP_Tune.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprofit\u001b[39m\u001b[39m'\u001b[39m,profit,\u001b[39m'\u001b[39m\u001b[39mn_trades\u001b[39m\u001b[39m'\u001b[39m,n_trades)\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/utils/pearl_utils.py:162\u001b[0m, in \u001b[0;36mtest_pearl_model\u001b[0;34m(agent, env, n_samples)\u001b[0m\n\u001b[1;32m    160\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m--> 162\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(exploit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    163\u001b[0m     action_result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    164\u001b[0m     agent\u001b[39m.\u001b[39mobserve(action_result)\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/pearl_agent.py:157\u001b[0m, in \u001b[0;36mPearlAgent.act\u001b[0;34m(self, exploit)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(safe_action_space, DiscreteActionSpace)\n\u001b[1;32m    155\u001b[0m     safe_action_space\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 157\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_learner\u001b[39m.\u001b[39;49mact(\n\u001b[1;32m    158\u001b[0m     subjective_state_to_be_used, safe_action_space, exploit\u001b[39m=\u001b[39;49mexploit  \u001b[39m# pyre-fixme[6]\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_latest_action \u001b[39m=\u001b[39m action\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/policy_learners/sequential_decision_making/deep_td_learning.py:206\u001b[0m, in \u001b[0;36mDeepTDLearning.act\u001b[0;34m(self, subjective_state, available_action_space, exploit)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m# TODO: Assumes gym action space.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m# Fix the available action space.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(available_action_space, DiscreteActionSpace)\n\u001b[0;32m--> 206\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mno_grad():\n\u001b[1;32m    207\u001b[0m     states_repeated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    208\u001b[0m         subjective_state\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m),\n\u001b[1;32m    209\u001b[0m         available_action_space\u001b[39m.\u001b[39mn,\n\u001b[1;32m    210\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     \u001b[39m# (action_space_size x state_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:154\u001b[0m, in \u001b[0;36m_NoParamDecoratorContextManager.__new__\u001b[0;34m(cls, orig_func)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_NoParamDecoratorContextManager\u001b[39;00m(_DecoratorContextManager):\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Allow a context manager to be used as a decorator without parentheses.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, orig_func\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    155\u001b[0m         \u001b[39mif\u001b[39;00m orig_func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(objective_function, n_trials=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n",
    "best_trials=study.best_trials\n",
    "best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials=study.best_trials\n",
    "best_trail=best_trials[3]\n",
    "best_params=best_trail.params\n",
    "reward_func=reward_functions[best_params.pop('reward_function')]\n",
    "train_env.reward_func=reward_func\n",
    "test_env.reward_func=reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo=best_params.pop('algorithm')\n",
    "\n",
    "learning_params={'learn_after_episode':best_params.pop('learn_after_episode'),\n",
    "                    'learning_steps':best_params.pop('learning_steps'),\n",
    "                    'n_epochs':best_params.pop('n_epochs'),\n",
    "                    }\n",
    "best_params['hidden_dims']=make_hidden_dims(n_layers=best_params.pop('n_layers'),n_units=best_params.pop('n_units'))\n",
    "best_params['lstm']=best_params.pop('lstm')\n",
    "best_params['action_space_dim']=len(train_env.positions)\n",
    "best_params['observation_space_dim']=train_env.observation_space.shape[0]\n",
    "if algo=='dqn':\n",
    "    agent=pearl_utils.create_dqn_model(**best_params)\n",
    "elif algo=='ddqn':\n",
    "    agent=pearl_utils.create_ddqn_model(**best_params)\n",
    "\n",
    "best_params,learning_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "agent=pearl_utils.train_pearl_model(agent,train_env,**learning_params)\n",
    "\n",
    "profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "\n",
    "agent=pearl_utils.train_pearl_model(agent,test_env,**learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
