{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_environments\n",
    "from utils import pearl_utils\n",
    "from configs import futures_defaults as defaults\n",
    "from utils.reward_functions import log_reward_function,cumulative_reward_function,sharpe_reward_function\n",
    "from utils. utils import make_hidden_dims\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from Pearl.pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from Pearl.pearl.utils.functional_utils.train_and_eval.online_learning import \\\n",
    "    online_learning\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=NeuralForecast.load('MultiHeadForecastingModel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_functions=[log_reward_function,cumulative_reward_function,sharpe_reward_function]\n",
    "train_env,test_env=make_environments.make_envs(reward_function=log_reward_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=train_env.positions\n",
    "train_env.action_space.n\n",
    "train_env.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hidden_dims(n_layers=3, n_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=pearl_utils.create_ddqn_model(\n",
    "\n",
    "        observation_space_dim=train_env.observation_space.shape[0], \n",
    "        action_space_dim=train_env.action_space.n,\n",
    "        hidden_dims=[64,64, 64], \n",
    "        training_rounds=20,\n",
    "        learning_rate = 0.001,\n",
    "        discount_factor = 0.99,\n",
    "        batch_size = 128,\n",
    "        target_update_freq = 10,\n",
    "        soft_update_tau = 0.75,  # a value of 1 indicates no soft updates\n",
    "        is_conservative = False,\n",
    "        conservative_alpha = False,\n",
    "        replay_buffer_size = 10_000,\n",
    "        lstm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.observation_space.shape,train_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=GymEnvironment(train_env)\n",
    "\n",
    "obs,action_space=env.reset()\n",
    "agent.reset(   obs, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done = False\n",
    "# while not done:\n",
    "#     action = agent.act(exploit=False)\n",
    "#     action_result = env.step(action)\n",
    "    \n",
    "#     agent.observe(action_result)\n",
    "#     loss=agent.learn()\n",
    "\n",
    "#     done = action_result.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = online_learning(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        # number_of_episodes=10,\n",
    "        number_of_steps=168,\n",
    "        print_every_x_episodes=2,   # print returns after every 10 episdoes\n",
    "        print_every_x_steps=1,   # print returns after every 10 episdoes\n",
    "        learn_every_k_steps=20,   # print returns after every 10 episdoes\n",
    "        learn_after_episode=False,\n",
    "        record_period=169,   # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "        seed=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33//1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective_function(trial):\n",
    " \n",
    "    reward_id=trial.suggest_categorical('reward_function', [0,1,2])\n",
    "    algo=trial.suggest_categorical('algorithm', ['dqn','ddqn'])    \n",
    "\n",
    "    # reward_id=0\n",
    "    \n",
    "    reward_func=reward_functions[reward_id]\n",
    "    train_env.reward_func=reward_func\n",
    "    test_env.reward_func=reward_func\n",
    "    \n",
    "    observation_space_dim=train_env.observation_space.shape[0]\n",
    "    action_space_dim=len(train_env.positions)\n",
    "    n_layers=trial.suggest_int('n_layers', 1, 3)\n",
    "    n_units=trial.suggest_categorical('n_units', [64,128,256,512])\n",
    "    \n",
    "    hidden_dims=make_hidden_dims(n_layers= n_layers, n_units=n_units)\n",
    "    \n",
    "    search_space={\n",
    "                'observation_space_dim': observation_space_dim,\n",
    "                'action_space_dim': action_space_dim,\n",
    "                'hidden_dims': hidden_dims,\n",
    "                'training_rounds': trial.suggest_int('training_rounds', 5, 30),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-4),\n",
    "                'discount_factor': trial.suggest_float('discount_factor', 0.8, 0.99), # gamma (greediness)\n",
    "                'batch_size': trial.suggest_categorical('batch_size', [64, 128]),\n",
    "                'target_update_freq': trial.suggest_categorical('target_update_freq', [1, 5, 10, 24]),\n",
    "                'soft_update_tau': trial.suggest_float('soft_update_tau', 0.1, .99), \n",
    "                'is_conservative': trial.suggest_categorical('is_conservative', [True, False]),\n",
    "                'lstm': trial.suggest_categorical('lstm', [True, False]),\n",
    "                'conservative_alpha': trial.suggest_float('conservative_alpha', 0.5, 1.0),\n",
    "                }\n",
    "\n",
    "    learning_space={'learn_after_episode':trial.suggest_categorical('learn_after_episode', [True, False]),\n",
    "                    'learning_steps':trial.suggest_int('learning_steps', 10, 89),\n",
    "                    'n_epochs':trial.suggest_categorical('n_epochs',[100,500]),\n",
    "                    }\n",
    "    #\n",
    "    # print('n_epochs',n_epochs)\n",
    "    if algo=='dqn':\n",
    "        agent=pearl_utils.create_dqn_model(**search_space)\n",
    "    elif algo=='ddqn':\n",
    "        agent=pearl_utils.create_ddqn_model(**search_space)\n",
    "\n",
    "        \n",
    "    agent=pearl_utils.train_pearl_model(agent,train_env,**learning_space)\n",
    "    profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "\n",
    "    print('profit',profit,'n_trades',n_trades)\n",
    "    return profit ,n_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=defaults.model_name\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "study = optuna.create_study(study_name=f\"pearl-{model_name}-hp-search\",\n",
    "                            directions=[\"maximize\", \"maximize\"],\n",
    "                            storage=\"sqlite:///pearl_hyper_parameters.sqlite3\",\n",
    "                            load_if_exists=True,\n",
    "                            sampler=TPESampler()\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective_function, n_trials=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n",
    "best_trials=study.best_trials\n",
    "best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials=study.best_trials\n",
    "best_trail=best_trials[3]\n",
    "best_params=best_trail.params\n",
    "reward_func=reward_functions[best_params.pop('reward_function')]\n",
    "train_env.reward_func=reward_func\n",
    "test_env.reward_func=reward_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo=best_params.pop('algorithm')\n",
    "\n",
    "learning_params={'learn_after_episode':best_params.pop('learn_after_episode'),\n",
    "                    'learning_steps':best_params.pop('learning_steps'),\n",
    "                    'n_epochs':best_params.pop('n_epochs'),\n",
    "                    }\n",
    "best_params['hidden_dims']=make_hidden_dims(n_layers=best_params.pop('n_layers'),n_units=best_params.pop('n_units'))\n",
    "best_params['lstm']=best_params.pop('lstm')\n",
    "if algo=='dqn':\n",
    "    agent=pearl_utils.create_dqn_model(**best_params)\n",
    "elif algo=='ddqn':\n",
    "    agent=pearl_utils.create_ddqn_model(**best_params)\n",
    "\n",
    "best_params,learning_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "agent=pearl_utils.train_pearl_model(agent,train_env,**learning_params)\n",
    "\n",
    "profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "\n",
    "agent=pearl_utils.train_pearl_model(agent,test_env,**learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "if -1 in [np.sign(p) for p in test_env.positions]:\n",
    "    market_type='Futures'\n",
    "else:\n",
    "    market_type='Spot'\n",
    "agent_path=f'Agent/{today}_pearl_{defaults.model_name}_model.pkl'\n",
    "\n",
    "pickle.dump(agent,open(agent_path,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
