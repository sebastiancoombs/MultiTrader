{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "from functools import lru_cache, partial\n",
    "from pprint import pprint\n",
    "import ray\n",
    "\n",
    "import gym_trading_env\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import MultiTrade\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gym_trading_env.downloader import download\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "from gym_trading_env.renderer import Renderer\n",
    "from IPython.display import display\n",
    "from ray import train, tune\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.utils import build_dataset, build_market_image,preprocess_data,stack_arrays\n",
    "from utils.forecast_utils import create_ts_preprocessor,create_ts_dataset\n",
    "\n",
    "from MultiTrade.environments import ForecastingTradingEnv,NeuralForecastingTradingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.deep_q_learning import DeepQLearning\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import OneHotActionTensorRepresentationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COIN_PAIRS=['BTC/USDT','ETH/USDT','SOL/USDT','BNB/USDT','XRP/USDT','ADA/USDT',\n",
    "            'ETH/BTC','SOL/ETH','BNB/ETH','XRP/ETH',\"ADA/ETH\",\n",
    "            'SOL/BTC','SOL/BNB',\n",
    "            'XRP/BTC','XRP/BNB',\n",
    "            'ADA/BTC','ADA/BNB',\n",
    "            ]\n",
    "target_pair='ETHUSDT'\n",
    "time_frame=\"1h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download(exchange_names = [\"binance\"],\n",
    "#     symbols= tqdm(COIN_PAIRS),\n",
    "#     timeframe= time_frame,\n",
    "#     dir = \"data\",\n",
    "#     since= datetime.datetime(year= 2022, month= 1, day=1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Seed set to 1\n",
      "Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "model=NeuralForecast.load(\"C:/Users/standard/Git/MultiTrader/forecaster_pretrain/neuralForecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=datetime.datetime(year= 2024, month= 2, day=1)\n",
    "split_date=start_date+datetime.timedelta(days=30)\n",
    "end_date=split_date+datetime.timedelta(days=30)\n",
    "target_pair='ETH/USDT'\n",
    "data=build_market_image(target_pair=target_pair,time_frame='1h',axis=0)\n",
    "data=data[data['symbol']==target_pair.replace('/','')].copy()\n",
    "data['ds']=data.index\n",
    "# hf_data=data.copy()\n",
    "\n",
    "# hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "# hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "\n",
    "# hf_train_data=hf_train_data.reset_index(level=0,drop=True).reset_index()\n",
    "# prepper=create_ts_preprocessor(hf_train_data)\n",
    "# hf_train_data=prepper.preprocess(hf_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date=datetime.datetime(year= 2024, month= 3, day=1)\n",
    "end_date=split_date+datetime.timedelta(days=14)\n",
    "\n",
    "# data=build_market_image(target_pair='ETH/USDT',time_frame='1h')\n",
    "\n",
    "hf_data=data.copy()\n",
    "hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "hf_test_data=hf_data.groupby('symbol').apply(lambda x: x[split_date:end_date])\n",
    "hf_test_data=hf_test_data.reset_index(level=0,drop=True).reset_index()\n",
    "# hf_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_envs(data,symbol='ETHUSDT',look_back=7):\n",
    "    start_date=datetime.datetime(year= 2024, month= 1, day=1)\n",
    "    split_date=datetime.datetime(year= 2024, month= 3, day=1)\n",
    "    end_date=split_date+datetime.timedelta(days=14)\n",
    "\n",
    "    \n",
    "    hf_data=data.copy()\n",
    "    \n",
    "    hf_train_data=hf_data.groupby('symbol').apply(lambda x: x[:split_date])\n",
    "    hf_test_data=hf_data.groupby('symbol').apply(lambda x: x[split_date:end_date])\n",
    "\n",
    "    hf_train_data=hf_train_data.reset_index(level=0,drop=True).reset_index()\n",
    "    hf_test_data=hf_test_data.reset_index(level=0,drop=True).reset_index()\n",
    "\n",
    "\n",
    "    trade_data=data[data['symbol']==symbol].copy()\n",
    "    train_data=trade_data[:split_date]\n",
    "    test_data=trade_data[split_date:end_date]\n",
    "\n",
    "\n",
    "    train_env = NeuralForecastingTradingEnv(\n",
    "                                        model=model,\n",
    "\n",
    "                                        name='ETHUSDT_train',\n",
    "                                        df = train_data, # Your dataset with your custom features\n",
    "                                        positions = [ -.5, 0, .5], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "                                        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "                                        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "                                        max_episode_duration=168,\n",
    "                                        verbose=0\n",
    "\n",
    "                                        )\n",
    "    \n",
    "    test_env = NeuralForecastingTradingEnv(\n",
    "                                        model=model,\n",
    "                                        # hf_data=hf_test_data,\n",
    "                                        \n",
    "                                        name='ETHUSDT_test',\n",
    "                                        df = test_data, # Your dataset with your custom features\n",
    "                                        positions = [ -.5, 0, .5], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "                                        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "                                        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "                                        max_episode_duration=168,\n",
    "                                        # verbose=0\n",
    "                                        \n",
    "                                    )\n",
    "    return train_env,test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 331 330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_env,test_env=get_train_test_envs(data,look_back=7)\n",
    "train_pearl_env=GymEnvironment(train_env)\n",
    "test_pearl_env=GymEnvironment(test_env)\n",
    "# obs,info=test_env.reset()\n",
    "# obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=train_env.reset()\n",
    "# env.action_space.sa\n",
    "done=False\n",
    "while not done:\n",
    "    act=train_pearl_env.action_space.sample()\n",
    "    act_result=train_pearl_env.step(act)\n",
    "    done=act_result.done\n",
    "# act_result.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs=train_env.reset()\n",
    "# # env.action_space.sa\n",
    "# for i in range(20):\n",
    "#     act=train_env.action_space.sample()\n",
    "#     act_result=train_env.step(act)\n",
    "# act_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dims': <ray.tune.search.sample.Categorical at 0x1992da61a00>,\n",
       " 'learning_rate': <ray.tune.search.sample.Float at 0x1992da61a90>,\n",
       " 'discount_factor': <ray.tune.search.sample.Float at 0x1992da61b20>,\n",
       " 'training_rounds': <ray.tune.search.sample.Categorical at 0x1992da61bb0>,\n",
       " 'batch_size': <ray.tune.search.sample.Categorical at 0x1992da61c40>,\n",
       " 'target_update_freq': <ray.tune.search.sample.Categorical at 0x1992d12fa30>,\n",
       " 'soft_update_tau': <ray.tune.search.sample.Float at 0x1992d12f580>,\n",
       " 'replay_buffer_size': <ray.tune.search.sample.Categorical at 0x1992d12f730>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space={\n",
    "        # \"look_back\" : tune.choice([7,14,21,30,45,60]),\n",
    "\n",
    "        \"hidden_dims\" : tune.choice([[64,64],[128,128],[256,256]]),\n",
    "\n",
    "        'learning_rate':tune.uniform(1e-6, 1e-2),\n",
    "\n",
    "        'discount_factor': tune.uniform(.5, 1),\n",
    "\n",
    "        'training_rounds': tune.choice([c for c in range(2,64,2)]),\n",
    "\n",
    "        'batch_size': tune.choice([64,128,256]),\n",
    "        \n",
    "        'target_update_freq':tune.choice([c for c in range(2,64,2)]),\n",
    "\n",
    "        'soft_update_tau': tune.uniform(1e-6, 1),  # a value of 1 indicates no soft updates\n",
    "        \n",
    "        \"replay_buffer_size\":tune.choice([c for c in range(10,200,10)]),\n",
    "        }\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pearl(pearl_env):\n",
    "    naked_env=pearl_env.env.unwrapped\n",
    "    value_history=naked_env.historical_info['portfolio_valuation']\n",
    "    x=np.arange(len(value_history))\n",
    "    y=value_history\n",
    "    plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective(config):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    hidden_dims=list(config[\"hidden_dims\"])\n",
    "    replay_buffer_size=config[\"replay_buffer_size\"]\n",
    "\n",
    "\n",
    "    # Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "    # Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "    # We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "    Q_network_DoubleDQN = VanillaQValueNetwork(state_dim=train_env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                                action_dim=train_env.action_space.n,                        # dimension of the action representation\n",
    "                                                hidden_dims=hidden_dims,                       # dimensions of the intermediate layers\n",
    "                                                output_dim=1)  \n",
    "    # Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "    # Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "    DoubleDQNagent = PearlAgent(\n",
    "                                policy_learner=DoubleDQN(\n",
    "                                                            state_dim=train_env.observation_space.shape[0],\n",
    "                                                            action_space=train_env.action_space,\n",
    "                                                            network_instance=Q_network_DoubleDQN,   # pass an instance of Q value network to the policy learner.\n",
    "                                                            action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "                                                                                                                                    max_number_actions=train_env.action_space.n\n",
    "                                                                                                                                ),\n",
    "                                                                                                                                \n",
    "                                                            **config\n",
    "                                                        ),\n",
    "                                replay_buffer=FIFOOffPolicyReplayBuffer(replay_buffer_size),\n",
    "                            ) \n",
    "    ## train dat bitch               \n",
    "    info = online_learning(\n",
    "                            agent=DoubleDQNagent ,\n",
    "                            env=train_pearl_env,\n",
    "                            number_of_episodes=2_000,\n",
    "                            print_every_x_episodes=500,   # print returns after every 10 episdoes\n",
    "                            learn_after_episode=True,    # updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "                            seed=0\n",
    "                            )\n",
    "    # plot_results(info)\n",
    "    agent=DoubleDQNagent\n",
    "    observation, action_space = test_pearl_env.reset()\n",
    "    agent.reset(observation, action_space)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(exploit=True)\n",
    "        action_result = test_pearl_env.step(action)\n",
    "        agent.observe(action_result)\n",
    "        agent.learn()\n",
    "        done = action_result.done\n",
    "\n",
    "    # plot_pearl(test_env)\n",
    "    score=action_result.info['portfolio_valuation']-1000\n",
    "    rew=action_result.info['reward']\n",
    "    \n",
    "    loss={\"score\": score,\n",
    "          \"reward\": rew,\n",
    "            \"_metric\": score}\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={x:y.sample() for x,y in search_space.items()}\n",
    "\n",
    "# objective(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = OptunaSearch()\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trial_str_creator(trial):\n",
    "    return \"{}_{}_trading_agent\".format(trial.trainable_name, trial.trial_id)\n",
    "\n",
    "hyperband = HyperBandScheduler(metric=\"score\", mode=\"max\")\n",
    "\n",
    "tune_config=tune.TuneConfig(num_samples=100,\n",
    "                            mode=\"max\",\n",
    "                            # search_alg='optuna',\n",
    "                            search_alg=algo,\n",
    "                                trial_name_creator=trial_str_creator,\n",
    "                                trial_dirname_creator=trial_str_creator,\n",
    "                                # scheduler=hyperband,\n",
    "                            )\n",
    "run_config=train.RunConfig(\n",
    "                            storage_path='C:/Users/standard/OneDrive/Documents/Git/MultiTrader/tune_results', \n",
    "                            name=\"DDQN_experiments\",\n",
    "                            \n",
    "                            \n",
    "                            )\n",
    "scaling_config=train.ScalingConfig(num_workers=4\n",
    "                                   \n",
    "                                   )\n",
    "objective_with_resources = tune.with_resources(objective, {\"cpu\": 0.5})\n",
    "# objective_with_resources = tune.with_resources(objective,{\"cpu\": 8,})\n",
    "objective_with_resources = tune.with_resources(objective,{\"cpu\": 8,'gpu':1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom.css\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom.css\n",
    "/* custom.css */\n",
    "\n",
    "/* Set background color for all widgets */\n",
    ".tuneStatus {\n",
    "    /* white background */\n",
    "    background-color: #ffffff;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"./custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load custom CSS file\n",
    "css = HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"./custom.css\">') # make sure the path is correct, it fails silently otherwise\n",
    "display(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner = tune.Tuner(objective_with_resources ,\n",
    "                   tune_config=tune_config,\n",
    "                   run_config=run_config,\n",
    "                #    scaling_config=scaling_config,\n",
    "                    \n",
    "                    param_space=search_space)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-11 09:35:19</td></tr>\n",
       "<tr><td>Running for: </td><td>00:26:48.73        </td></tr>\n",
       "<tr><td>Memory:      </td><td>15.8/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 8.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  discount_factor</th><th>hidden_dims  </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  replay_buffer_size</th><th style=\"text-align: right;\">  soft_update_tau</th><th style=\"text-align: right;\">  target_update_freq</th><th style=\"text-align: right;\">  training_rounds</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    score</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  _metric</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_c5b38d2e_trading_agent</td><td>RUNNING   </td><td>127.0.0.1:31684</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         0.694806</td><td>[128, 128]   </td><td style=\"text-align: right;\">     0.00335294</td><td style=\"text-align: right;\">                  50</td><td style=\"text-align: right;\">        0.502854 </td><td style=\"text-align: right;\">                  52</td><td style=\"text-align: right;\">               48</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>objective_289141da_trading_agent</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         0.847036</td><td>[256, 256]   </td><td style=\"text-align: right;\">     0.00809229</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">        0.952772 </td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">               14</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">         </td></tr>\n",
       "<tr><td>objective_ae372e64_trading_agent</td><td>TERMINATED</td><td>127.0.0.1:10628</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">         0.995827</td><td>[128, 128]   </td><td style=\"text-align: right;\">     0.0077878 </td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">        0.0233487</td><td style=\"text-align: right;\">                  60</td><td style=\"text-align: right;\">               56</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1164.41</td><td style=\"text-align: right;\">-0.050005</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">-0.050005</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (52 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 100, step 16700, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.008408576770307263\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 200, step 33400, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.0017522163943795022\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 300, step 50100, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.008924095203838078\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 400, step 66800, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.021158812884095823\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 500, step 83500, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.0035516101161192637\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 600, step 100200, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.002105842941091396\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 700, step 116900, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.010847383804502897\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 800, step 133600, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.02282008179827244\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 900, step 150300, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.01841267328882168\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1000, step 166945, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.02100347831947147\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1100, step 183645, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.01361451027696603\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1200, step 200345, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.0819342738504929\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1300, step 217045, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.11958340028650127\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1400, step 233745, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.0013827937073074281\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1500, step 250445, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.00038674554889439605\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1600, step 267145, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -6.848389239166863e-05\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1700, step 283845, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.0010539173999859486\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1800, step 300545, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.015656359661079478\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 1900, step 317245, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: -0.005092214509204496\n",
      "\u001b[36m(objective pid=10628)\u001b[0m episode 2000, step 333945, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=10628)\u001b[0m return: 0.0038244169154495467\n",
      "\u001b[36m(objective pid=10628)\u001b[0m Market Return : 18.59%   |   Portfolio Return : -0.01%   |   \n",
      "\u001b[36m(objective pid=10628)\u001b[0m {'score': -0.0500050005000503, 'reward': 0.0, '_metric': -0.0500050005000503}\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 100, step 16700, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: -0.13314272330899257\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 200, step 33400, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: 0.11587707512808265\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 300, step 50100, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: 0.0147825867679785\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 400, step 66800, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: 0.10381667985348031\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 500, step 83500, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: -0.017964830862638337\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 600, step 100200, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: 0.28828218493708846\n",
      "\u001b[36m(objective pid=31684)\u001b[0m episode 700, step 116900, agent=PearlAgent with DoubleDQN, FIFOOffPolicyReplayBuffer, env=CustomGymEnvironment\n",
      "\u001b[36m(objective pid=31684)\u001b[0m return: 0.009088772163522663\n"
     ]
    }
   ],
   "source": [
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit\n",
    "df_results = results.get_dataframe()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=results.get_best_result(metric=\"score\", mode=\"max\").config\n",
    "best_params\n",
    "# objective(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.env.unwrapped.save_for_render(dir = \"test_render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "renderer = Renderer(render_logs_dir = \"test_render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Custom Metrics (Annualized metrics)\n",
    "renderer.add_metric(\n",
    "    name = \"Annual Market Return\",\n",
    "    function = lambda df : f\"{ ((df['close'].iloc[-1] / df['close'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%\"\n",
    ")\n",
    "renderer.add_metric(\n",
    "        name = \"Annual Portfolio Return\",\n",
    "        function = lambda df : f\"{((df['portfolio_valuation'].iloc[-1] / df['portfolio_valuation'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%\"\n",
    ")\n",
    "\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
