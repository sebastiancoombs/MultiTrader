{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_environments\n",
    "from utils import pearl_utils\n",
    "from configs  import defaults\n",
    "\n",
    "from utils.reward_functions import log_reward_function,cumulative_reward_function,sharpe_reward_function\n",
    "from utils. utils import make_hidden_dims\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from Pearl.pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from Pearl.pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3=boto3.client('s3', aws_access_key_id=Keys.AWS_ACCESS_KEY, aws_secret_access_key=Keys.AWS_SECRET_KEY)\n",
    "# agent_path=f'Agent/pearl_{defaults.model_name}_model.pkl'\n",
    "\n",
    "# s3.download_file('coinbasetradehistory',f'pearl_{defaults.model_name}_model.pkl',agent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=NeuralForecast.load('MultiHeadForecastingModel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 12\n",
      "Seed set to 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_forex/oanda-USDJPY-1h.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 10602.32it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86deecf598ad423a8c9b8111a2c48458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7691de5159b74047a8b15f1269fbfa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-26 12:00:00 2024-11-25 12:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24,), np.int64(2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_functions=[log_reward_function,cumulative_reward_function,sharpe_reward_function]\n",
    "train_env,test_env=make_environments.make_envs(reward_function=log_reward_function)\n",
    "study_name=f\"{defaults.model_name}\"\n",
    "storage_name=\"sqlite:///PearlHPTuning.sqlite3\"\n",
    "\n",
    "test_env.observation_space.shape,train_env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "study = optuna.load_study(study_name=study_name,\n",
    "                            storage=storage_name,\n",
    "\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=45, state=TrialState.COMPLETE, values=[1191.6558529692093, 20.4], datetime_start=datetime.datetime(2024, 11, 24, 2, 9, 29, 249544), datetime_complete=datetime.datetime(2024, 11, 24, 2, 14, 53, 613837), params={'reward_function': 0, 'algorithm': 'dqn', 'n_layers': 1, 'n_units': 256, 'training_rounds': 21, 'learning_rate': 2.8764743562927777e-05, 'discount_factor': 0.911339518183664, 'batch_size': 64, 'target_update_freq': 5, 'soft_update_tau': 0.30917867193935245, 'is_conservative': False, 'lstm': True, 'conservative_alpha': 0.5255327383263214, 'learn_after_episode': True, 'learning_steps': 51, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=64, value=None),\n",
       " FrozenTrial(number=51, state=TrialState.COMPLETE, values=[1091.4864788100776, 511.6], datetime_start=datetime.datetime(2024, 11, 24, 2, 43, 19, 578629), datetime_complete=datetime.datetime(2024, 11, 24, 2, 50, 10, 546809), params={'reward_function': 0, 'algorithm': 'dqn', 'n_layers': 1, 'n_units': 128, 'training_rounds': 28, 'learning_rate': 4.2165519977742505e-05, 'discount_factor': 0.9373318151729744, 'batch_size': 64, 'target_update_freq': 24, 'soft_update_tau': 0.49543218166677083, 'is_conservative': False, 'lstm': True, 'conservative_alpha': 0.5943095939411217, 'learn_after_episode': True, 'learning_steps': 55, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=70, value=None),\n",
       " FrozenTrial(number=53, state=TrialState.COMPLETE, values=[1187.7147489399895, 21.2], datetime_start=datetime.datetime(2024, 11, 24, 2, 56, 11, 958359), datetime_complete=datetime.datetime(2024, 11, 24, 3, 3, 2, 826177), params={'reward_function': 2, 'algorithm': 'dqn', 'n_layers': 3, 'n_units': 128, 'training_rounds': 28, 'learning_rate': 5.34304347683492e-05, 'discount_factor': 0.9406775246087808, 'batch_size': 64, 'target_update_freq': 24, 'soft_update_tau': 0.5805151989304606, 'is_conservative': False, 'lstm': True, 'conservative_alpha': 0.941728114103827, 'learn_after_episode': True, 'learning_steps': 55, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=72, value=None),\n",
       " FrozenTrial(number=66, state=TrialState.COMPLETE, values=[1157.2716904159802, 64.8], datetime_start=datetime.datetime(2024, 11, 24, 5, 15, 2, 446081), datetime_complete=datetime.datetime(2024, 11, 24, 5, 21, 56, 849084), params={'reward_function': 0, 'algorithm': 'dqn', 'n_layers': 2, 'n_units': 128, 'training_rounds': 28, 'learning_rate': 3.502195725430407e-05, 'discount_factor': 0.9447058427910774, 'batch_size': 128, 'target_update_freq': 24, 'soft_update_tau': 0.5996751853354005, 'is_conservative': True, 'lstm': True, 'conservative_alpha': 0.6505399516357236, 'learn_after_episode': True, 'learning_steps': 72, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=85, value=None),\n",
       " FrozenTrial(number=82, state=TrialState.COMPLETE, values=[530.6757656729593, 6261.6], datetime_start=datetime.datetime(2024, 11, 24, 7, 45, 50, 907009), datetime_complete=datetime.datetime(2024, 11, 24, 7, 52, 13, 714153), params={'reward_function': 0, 'algorithm': 'dqn', 'n_layers': 1, 'n_units': 64, 'training_rounds': 8, 'learning_rate': 4.232514535603427e-05, 'discount_factor': 0.9654166988566071, 'batch_size': 64, 'target_update_freq': 24, 'soft_update_tau': 0.9350127594607229, 'is_conservative': False, 'lstm': True, 'conservative_alpha': 0.6036362569632845, 'learn_after_episode': True, 'learning_steps': 44, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=101, value=None),\n",
       " FrozenTrial(number=98, state=TrialState.COMPLETE, values=[1187.58085077912, 60.4], datetime_start=datetime.datetime(2024, 11, 24, 9, 16, 28, 871600), datetime_complete=datetime.datetime(2024, 11, 24, 9, 21, 59, 700893), params={'reward_function': 1, 'algorithm': 'dqn', 'n_layers': 1, 'n_units': 64, 'training_rounds': 12, 'learning_rate': 5.2188235268080546e-05, 'discount_factor': 0.9623992040133226, 'batch_size': 64, 'target_update_freq': 24, 'soft_update_tau': 0.6710603668114904, 'is_conservative': False, 'lstm': True, 'conservative_alpha': 0.8978216863599142, 'learn_after_episode': True, 'learning_steps': 55, 'n_epochs': 100}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'reward_function': CategoricalDistribution(choices=(0, 1, 2)), 'algorithm': CategoricalDistribution(choices=('dqn', 'ddqn')), 'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'n_units': CategoricalDistribution(choices=(64, 128, 256, 512)), 'training_rounds': IntDistribution(high=30, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-06, step=None), 'discount_factor': FloatDistribution(high=0.99, log=False, low=0.8, step=None), 'batch_size': CategoricalDistribution(choices=(64, 128)), 'target_update_freq': CategoricalDistribution(choices=(1, 5, 10, 24)), 'soft_update_tau': FloatDistribution(high=0.99, log=False, low=0.1, step=None), 'is_conservative': CategoricalDistribution(choices=(True, False)), 'lstm': CategoricalDistribution(choices=(True, False)), 'conservative_alpha': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'learn_after_episode': CategoricalDistribution(choices=(True, False)), 'learning_steps': IntDistribution(high=89, log=False, low=10, step=1), 'n_epochs': CategoricalDistribution(choices=(100, 500))}, trial_id=117, value=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n",
    "best_trials=study.best_trials\n",
    "best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent/pearl_USDJPY_model.pkl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "agent_path=f'Agent/pearl_{defaults.model_name}_model.pkl'\n",
    "\n",
    "agent_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901151a85c074af2ade06984ef819334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Return AVG Profit: 838.1052875360961, AVG Number of Trades: 20.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5536b0f0d34992861efa5d1ec9bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:54<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2198e03d2448b8a7bf9a5b534e4aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:11<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Return AVG Profit: 1155.838142126782, AVG Number of Trades: 20.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d272cdb2e62446bd9aacfb98554b02b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:11<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d40359b95d6468aaef0ee6b2b6579b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:16<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Return AVG Profit: 830.6017718906991, AVG Number of Trades: 23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c63e1bd06f94e63989286f970b378c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:18<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c9aed6e9224a0ab63148c75c7cc5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:29<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Return AVG Profit: 1148.8736471134691, AVG Number of Trades: 20.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8bbbc7794347ff9011180d250f6901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [03:27<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768667c9dfcc4f648100f3f40ed0ae2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:28<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Return AVG Profit: 861.9031110465813, AVG Number of Trades: 17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b138f2cbe18d4b8a80115512def493e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:48<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ba5c093c1c475a9375c6ee6c44ad72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [02:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(best_trials)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     agent,learning_params\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39mload_agent_from_study(study_path\u001b[39m=\u001b[39mstorage_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                             study_name\u001b[39m=\u001b[39mstudy_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                             action_space_dim\u001b[39m=\u001b[39mtrain_env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                             observation_space_dim\u001b[39m=\u001b[39mtrain_env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                             version\u001b[39m=\u001b[39mi)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     agent,profit,n_trades\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39;49mtrain_production_agent(agent,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                 learning_params,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                 train_env\u001b[39m=\u001b[39;49mtrain_env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                 test_env\u001b[39m=\u001b[39;49mtest_env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                 save_path\u001b[39m=\u001b[39;49magent_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     best_agents[profit]\u001b[39m=\u001b[39magent\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m agent\u001b[39m=\u001b[39mbest_agents[\u001b[39mmax\u001b[39m(best_agents)]\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/utils/pearl_utils.py:216\u001b[0m, in \u001b[0;36mtrain_production_agent\u001b[0;34m(agent, learning_params, train_env, test_env, save_path)\u001b[0m\n\u001b[1;32m    213\u001b[0m test_env\u001b[39m.\u001b[39mreward_function\u001b[39m=\u001b[39mreward_func\n\u001b[1;32m    214\u001b[0m agent\u001b[39m=\u001b[39mtrain_pearl_model(agent,train_env,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlearning_params)\n\u001b[0;32m--> 216\u001b[0m profit,n_trades\u001b[39m=\u001b[39mtest_pearl_model(agent,test_env)\n\u001b[1;32m    217\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesting Return AVG Profit: \u001b[39m\u001b[39m{\u001b[39;00mprofit\u001b[39m}\u001b[39;00m\u001b[39m, AVG Number of Trades: \u001b[39m\u001b[39m{\u001b[39;00mn_trades\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m agent\u001b[39m=\u001b[39mtrain_pearl_model(agent,test_env,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlearning_params)\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/utils/pearl_utils.py:164\u001b[0m, in \u001b[0;36mtest_pearl_model\u001b[0;34m(agent, env, n_samples)\u001b[0m\n\u001b[1;32m    162\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(exploit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m action_result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> 164\u001b[0m agent\u001b[39m.\u001b[39;49mobserve(action_result)\n\u001b[1;32m    165\u001b[0m \u001b[39m# no agent.learn() while testing\u001b[39;00m\n\u001b[1;32m    166\u001b[0m done \u001b[39m=\u001b[39m action_result\u001b[39m.\u001b[39mdone\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/pearl_agent.py:179\u001b[0m, in \u001b[0;36mPearlAgent.observe\u001b[0;34m(self, action_result)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_latest_action \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_space \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer\u001b[39m.\u001b[39;49mpush(\n\u001b[1;32m    180\u001b[0m     \u001b[39m# pyre-fixme[6]: this can be removed when tabular Q learning test uses tensors\u001b[39;49;00m\n\u001b[1;32m    181\u001b[0m     state\u001b[39m=\u001b[39;49mcurrent_history,\n\u001b[1;32m    182\u001b[0m     action\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_latest_action,\n\u001b[1;32m    183\u001b[0m     reward\u001b[39m=\u001b[39;49maction_result\u001b[39m.\u001b[39;49mreward,\n\u001b[1;32m    184\u001b[0m     \u001b[39m# pyre-fixme[6]: this can be removed when tabular Q learning test uses tensors\u001b[39;49;00m\n\u001b[1;32m    185\u001b[0m     next_state\u001b[39m=\u001b[39;49mnew_history,\n\u001b[1;32m    186\u001b[0m     curr_available_actions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_space,  \u001b[39m# curr_available_actions\u001b[39;49;00m\n\u001b[1;32m    187\u001b[0m     next_available_actions\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    188\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_space\n\u001b[1;32m    189\u001b[0m         \u001b[39mif\u001b[39;49;00m action_result\u001b[39m.\u001b[39;49mavailable_action_space \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    190\u001b[0m         \u001b[39melse\u001b[39;49;00m action_result\u001b[39m.\u001b[39;49mavailable_action_space\n\u001b[1;32m    191\u001b[0m     ),  \u001b[39m# next_available_actions\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m     terminated\u001b[39m=\u001b[39;49maction_result\u001b[39m.\u001b[39;49mterminated,\n\u001b[1;32m    193\u001b[0m     max_number_actions\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    194\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_learner\u001b[39m.\u001b[39;49maction_representation_module\u001b[39m.\u001b[39;49mmax_number_actions\n\u001b[1;32m    195\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_learner\u001b[39m.\u001b[39;49m_is_action_continuous\n\u001b[1;32m    196\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    197\u001b[0m     ),  \u001b[39m# max number of actions for discrete action space\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     cost\u001b[39m=\u001b[39;49maction_result\u001b[39m.\u001b[39;49mcost,\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_space \u001b[39m=\u001b[39m (\n\u001b[1;32m    202\u001b[0m     action_result\u001b[39m.\u001b[39mavailable_action_space\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m action_result\u001b[39m.\u001b[39mavailable_action_space \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_space\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subjective_state \u001b[39m=\u001b[39m new_subjective_state\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/replay_buffers/tensor_based_replay_buffer.py:87\u001b[0m, in \u001b[0;36mTensorBasedReplayBuffer.push\u001b[0;34m(self, state, action, reward, terminated, curr_available_actions, next_state, next_available_actions, max_number_actions, cost)\u001b[0m\n\u001b[1;32m     72\u001b[0m     (\n\u001b[1;32m     73\u001b[0m         curr_available_actions_tensor_with_padding,\n\u001b[1;32m     74\u001b[0m         curr_unavailable_actions_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     (\n\u001b[1;32m     85\u001b[0m         curr_available_actions_tensor_with_padding,\n\u001b[1;32m     86\u001b[0m         curr_unavailable_actions_mask,\n\u001b[0;32m---> 87\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_action_tensor_and_mask(\n\u001b[1;32m     88\u001b[0m         max_number_actions, curr_available_actions\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     (\n\u001b[1;32m     92\u001b[0m         next_available_actions_tensor_with_padding,\n\u001b[1;32m     93\u001b[0m         next_unavailable_actions_mask,\n\u001b[1;32m     94\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_action_tensor_and_mask(\n\u001b[1;32m     95\u001b[0m         max_number_actions, next_available_actions\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[39m# Transitions require a \"batch\" dimension in the tensors,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39m# so we add a dimension of size 1.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/replay_buffers/tensor_based_replay_buffer.py:238\u001b[0m, in \u001b[0;36mTensorBasedReplayBuffer.create_action_tensor_and_mask\u001b[0;34m(max_number_actions, available_action_space)\u001b[0m\n\u001b[1;32m    233\u001b[0m available_actions_tensor \u001b[39m=\u001b[39m available_action_space\u001b[39m.\u001b[39mactions_batch\n\u001b[1;32m    234\u001b[0m available_actions_tensor_with_padding[: available_action_space\u001b[39m.\u001b[39mn, :] \u001b[39m=\u001b[39m (\n\u001b[1;32m    235\u001b[0m     available_actions_tensor\n\u001b[1;32m    236\u001b[0m )\n\u001b[0;32m--> 238\u001b[0m unavailable_actions_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\n\u001b[1;32m    239\u001b[0m     (max_number_actions,)\n\u001b[1;32m    240\u001b[0m )  \u001b[39m# (action_space_size)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m unavailable_actions_mask[available_action_space\u001b[39m.\u001b[39mn :] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    242\u001b[0m unavailable_actions_mask \u001b[39m=\u001b[39m unavailable_actions_mask\u001b[39m.\u001b[39mbool()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_agents={}\n",
    "for i in range(len(best_trials)):\n",
    "    agent,learning_params=pearl_utils.load_agent_from_study(study_path=storage_name,\n",
    "                                            study_name=study_name,\n",
    "                                            action_space_dim=train_env.action_space.n,\n",
    "                                            observation_space_dim=train_env.observation_space.shape[0],\n",
    "                                            version=i)\n",
    "    \n",
    "    agent,profit,n_trades=pearl_utils.train_production_agent(agent,\n",
    "                                learning_params,\n",
    "                                train_env=train_env,\n",
    "                                test_env=test_env,\n",
    "                                save_path=agent_path)\n",
    "    best_agents[profit]=agent\n",
    "agent=best_agents[max(best_agents)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent,learning_params=pearl_utils.load_agent_from_study(study_path=\"sqlite:///pearl_hyper_parameters.sqlite3\",\n",
    "#                                             study_name='pearl-2024-11-12-hp-search',\n",
    "#                                             action_space_dim=2,\n",
    "#                                             observation_space_dim=30,\n",
    "#                                             version=1)\n",
    "\n",
    "# agent=pearl_utils.train_production_agent(agent,\n",
    "#                              learning_params,\n",
    "#                              train_env=train_env,\n",
    "#                              test_env=train_env,\n",
    "#                              save_path=agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=best_agents[max(best_agents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({np.float64(838.1052875360961): <Pearl.pearl.pearl_agent.PearlAgent at 0x379d1e350>,\n",
       "  np.float64(1155.838142126782): <Pearl.pearl.pearl_agent.PearlAgent at 0x37db61150>,\n",
       "  np.float64(830.6017718906991): <Pearl.pearl.pearl_agent.PearlAgent at 0x3f74741c0>,\n",
       "  np.float64(1148.8736471134691): <Pearl.pearl.pearl_agent.PearlAgent at 0x3dd5f2aa0>,\n",
       "  np.float64(861.9031110465813): <Pearl.pearl.pearl_agent.PearlAgent at 0x468b5baf0>},\n",
       " np.float64(1155.838142126782))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_agents,max(best_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m profit,n_trades\u001b[39m=\u001b[39mpearl_utils\u001b[39m.\u001b[39;49mtest_pearl_model(agent,test_env)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/Git/MultiTrader/Pearl_Fit_Best_Agent.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesting Return AVG Profit: \u001b[39m\u001b[39m{\u001b[39;00mprofit\u001b[39m}\u001b[39;00m\u001b[39m, AVG Number of Trades: \u001b[39m\u001b[39m{\u001b[39;00mn_trades\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/utils/pearl_utils.py:164\u001b[0m, in \u001b[0;36mtest_pearl_model\u001b[0;34m(agent, env, n_samples)\u001b[0m\n\u001b[1;32m    162\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(exploit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m action_result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> 164\u001b[0m agent\u001b[39m.\u001b[39;49mobserve(action_result)\n\u001b[1;32m    165\u001b[0m \u001b[39m# no agent.learn() while testing\u001b[39;00m\n\u001b[1;32m    166\u001b[0m done \u001b[39m=\u001b[39m action_result\u001b[39m.\u001b[39mdone\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/pearl_agent.py:170\u001b[0m, in \u001b[0;36mPearlAgent.observe\u001b[0;34m(self, action_result)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobserve\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    167\u001b[0m     action_result: ActionResult,\n\u001b[1;32m    168\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     current_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_summarization_module\u001b[39m.\u001b[39mget_history()\n\u001b[0;32m--> 170\u001b[0m     new_subjective_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_subjective_state(action_result\u001b[39m.\u001b[39;49mobservation)\n\u001b[1;32m    171\u001b[0m     new_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_summarization_module\u001b[39m.\u001b[39mget_history()\n\u001b[1;32m    173\u001b[0m     \u001b[39m# TODO: define each push with a uuid\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# TODO: currently assumes the same action space across all steps\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[39m# need to modify ActionResults\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/pearl_agent.py:257\u001b[0m, in \u001b[0;36mPearlAgent._update_subjective_state\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    246\u001b[0m     latest_action_representation \u001b[39m=\u001b[39m (\n\u001b[1;32m    247\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_learner\u001b[39m.\u001b[39maction_representation_module(\n\u001b[1;32m    248\u001b[0m             torch\u001b[39m.\u001b[39mas_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_latest_action)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    251\u001b[0m observation_to_be_used \u001b[39m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     torch\u001b[39m.\u001b[39mas_tensor(observation)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_learner\u001b[39m.\u001b[39mrequires_tensors  \u001b[39m# temporary fix before abstract interfaces\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39melse\u001b[39;00m observation\n\u001b[1;32m    255\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_summarization_module\u001b[39m.\u001b[39;49msummarize_history(\n\u001b[1;32m    258\u001b[0m     observation_to_be_used, latest_action_representation\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Git/MultiTrader/Pearl/pearl/history_summarization_modules/lstm_history_summarization_module.py:78\u001b[0m, in \u001b[0;36mLSTMHistorySummarizationModule.summarize_history\u001b[0;34m(self, observation, action)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39massert\u001b[39;00m observation\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m action\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     70\u001b[0m     [\n\u001b[1;32m     71\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m1\u001b[39m:, :],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m out, (_, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory)\n\u001b[1;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pearlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\n\u001b[1;32m   1124\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1125\u001b[0m         hx,\n\u001b[1;32m   1126\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights,\n\u001b[1;32m   1127\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1128\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m   1129\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout,\n\u001b[1;32m   1130\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1131\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[1;32m   1132\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first,\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "print(f\"Testing Return AVG Profit: {profit}, AVG Number of Trades: {n_trades}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(agent,open(agent_path,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(agent.policy_learner.state_dict(),open(agent_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights=pickle.load(open(agent_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy_learner.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agent,learning_params=pearl_utils.load_agent_from_study(study_path=\"sqlite:///pearl_hyper_parameters.sqlite3\",\n",
    "#                                         study_name='pearl-2024-11-12-hp-search',\n",
    "#                                         action_space_dim=2,\n",
    "#                                         observation_space_dim=30)\n",
    "# agent=load_agent_weights(agent,weight_path=agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit,n_trades=pearl_utils.test_pearl_model(agent,test_env)\n",
    "print(f\"Testing Return AVG Profit: {profit}, AVG Number of Trades: {n_trades}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
